# ğŸš€ DÃ©marrage rapide - Gateway Server avec qwen-code

Guide rapide pour utiliser le gateway server avec qwen-code et les outils.

## ğŸ“¦ PrÃ©requis

1. **Serveur LLM** : Un moteur llama.cpp en cours d'exÃ©cution
   ```bash
   # Le serveur doit Ãªtre accessible sur http://localhost:12434
   ```

2. **qwen-code** : Installer qwen-code
   ```bash
   npm install -g @qwen-code/qwen-code
   ```

## ğŸ¯ DÃ©marrage en 3 Ã©tapes

### Ã‰tape 1 : DÃ©marrer le gateway server

```bash
cd samples/85-gateway-server-agent-crew
go run main.go
```

Vous devriez voir :
```
ğŸš€ Gateway crew server starting on http://localhost:8080
ğŸ“¡ OpenAI-compatible endpoint: POST /v1/chat/completions
ğŸ‘¥ Crew agents: coder, thinker, generic
ğŸ”§ Tools mode: passthrough (client-side)
```

### Ã‰tape 2 : Configurer les variables d'environnement

```bash
export OPENAI_BASE_URL=http://localhost:8080/v1
export OPENAI_API_KEY=none
export OPENAI_MODEL=crew
```

### Ã‰tape 3 : Lancer qwen-code

```bash
qwen-code
```

C'est tout ! ğŸ‰

## âœ… Test rapide

Une fois qwen-code lancÃ©, testez avec :

```
You: Write a hello world in Go
```

Le gateway devrait automatiquement router vers l'agent **coder** et gÃ©nÃ©rer le code.

## ğŸ› ï¸ Utilisation des outils

Qwen-code gÃ¨re automatiquement les outils. Par exemple :

```
You: Read the file package.json and tell me the version
```

Qwen-code va :
1. DÃ©clarer l'outil `read_file` au gateway
2. Le LLM dÃ©cide d'utiliser l'outil
3. Qwen-code exÃ©cute la lecture du fichier
4. Le LLM gÃ©nÃ¨re la rÃ©ponse avec le contenu

**Tout cela est automatique !** ğŸ¯

## ğŸ” Modes de fonctionnement

### Mode actuel : **Passthrough** (dÃ©faut)

- âœ… Qwen-code gÃ¨re les outils
- âœ… ExÃ©cution cÃ´tÃ© client
- âœ… SÃ©curitÃ© maximale
- âœ… FlexibilitÃ© totale

### Mode alternatif : **Auto-Execute**

Pour activer le mode Auto-Execute (outils cÃ´tÃ© serveur), voir [README-tools.md](README-tools.md#configuration-avancÃ©e).

## ğŸ“Š Agents disponibles

Le gateway route automatiquement vers le bon agent :

| Agent | DÃ©clencheurs | Use case |
|-------|-------------|----------|
| **coder** | coding, programming, development, code, software | Code, debug, refactoring |
| **thinker** | philosophy, thinking, ideas, psychology, math, science | RÃ©flexion, analyse, problÃ¨mes complexes |
| **generic** | tout le reste | Questions gÃ©nÃ©rales |

## ğŸ§ª Tester avec curl

Si vous voulez tester sans qwen-code :

```bash
# Test simple
curl http://localhost:8080/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "crew",
    "messages": [
      {"role": "user", "content": "Hello!"}
    ],
    "stream": false
  }' | jq .
```

Pour des exemples plus avancÃ©s avec outils :

```bash
./examples-tools.sh
```

## ğŸ› DÃ©pannage

### Erreur : "connection refused"

**Cause :** Le serveur LLM n'est pas dÃ©marrÃ©

**Solution :** VÃ©rifiez que llama.cpp tourne sur `localhost:12434`

### Erreur : "model not found"

**Cause :** Les modÃ¨les ne sont pas tÃ©lÃ©chargÃ©s

**Solution :** VÃ©rifiez que les modÃ¨les dans `main.go` sont disponibles :
- `hf.co/qwen/qwen2.5-coder-3b-instruct-gguf:q4_k_m`
- `hf.co/menlo/lucy-gguf:q4_k_m`
- `hf.co/menlo/jan-nano-gguf:q4_k_m`

### Qwen-code ne trouve pas le modÃ¨le

**Cause :** Variable `OPENAI_MODEL` non dÃ©finie

**Solution :**
```bash
export OPENAI_MODEL=crew
```

### Les outils ne fonctionnent pas

**Cause :** Qwen-code doit Ãªtre configurÃ© pour utiliser les outils

**Solution :** VÃ©rifiez la configuration de qwen-code pour les outils disponibles

## ğŸ“š Documentation complÃ¨te

- [README-tools.md](README-tools.md) - Guide complet sur les outils
- [examples-tools.sh](examples-tools.sh) - Exemples pratiques avec curl
- [test.sh](test.sh) - Suite de tests du gateway

## ğŸ¨ Personnalisation

### Changer le port

Modifiez dans `main.go` :

```go
gatewayserver.WithPort(8080), // Changez 8080 par votre port
```

### Modifier les agents

Ajoutez, supprimez ou modifiez les agents dans la fonction `main()` :

```go
agentCrew := map[string]*chat.Agent{
    "coder":   coderAgent,
    "thinker": thinkerAgent,
    "generic": genericAgent,
    // Ajoutez vos agents ici
}
```

### Personnaliser le routage

Modifiez la fonction `matchAgentFunction` pour changer les rÃ¨gles de routage :

```go
matchAgentFunction := func(currentAgentId, topic string) string {
    switch strings.ToLower(topic) {
    case "coding":
        return "coder"
    case "philosophy":
        return "thinker"
    // Ajoutez vos rÃ¨gles ici
    default:
        return "generic"
    }
}
```

## ğŸ’¡ Conseils d'utilisation

### 1. Toujours prÃ©ciser le contexte

âŒ Mauvais : "Fix this"
âœ… Bon : "Fix the syntax error in the Go function reverseString"

### 2. Utiliser les bons mots-clÃ©s pour le routage

- Pour du code : "write", "debug", "fix", "code", "function"
- Pour de la rÃ©flexion : "explain", "why", "philosophy", "analyze"
- Pour des questions gÃ©nÃ©rales : tout le reste

### 3. Profiter des outils de qwen-code

Qwen-code a accÃ¨s Ã  votre systÃ¨me de fichiers local, utilisez-le !

```
You: Read all .go files in the current directory and find potential bugs
```

## ğŸŒŸ FonctionnalitÃ©s avancÃ©es

### Compression automatique

Le gateway compresse automatiquement l'historique quand il dÃ©passe 7000 caractÃ¨res :

```go
gatewayserver.WithCompressorAgentAndContextSize(compressorAgent, 7000)
```

### Orchestration multi-agents

L'orchestrateur analyse automatiquement le sujet et route vers le bon agent :

```go
gatewayserver.WithOrchestratorAgent(orchestratorAgent)
```

### Hooks de cycle de vie

Vous pouvez ajouter des hooks avant/aprÃ¨s chaque requÃªte :

```go
gatewayserver.BeforeCompletion(func(agent *gatewayserver.GatewayServerAgent) {
    fmt.Printf("ğŸ“¥ Request received\n")
})
```

## ğŸ”— Liens utiles

- [Nova SDK Documentation](../../README.md)
- [Qwen Code GitHub](https://github.com/QwenLM/qwen-code)
- [OpenAI API Reference](https://platform.openai.com/docs/api-reference)

---

**Besoin d'aide ?** Consultez [README-tools.md](README-tools.md) pour plus de dÃ©tails.
